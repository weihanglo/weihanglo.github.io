<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬
本文以重點整理的方式，先介紹目前常見的 Autoscaler，再介紹一些防止 pod 被亂殺的 config。
（撰於 2020-03-23，基於 Kubernetes 1.17，但 Api Versions 太多請自行查閱手冊）
讓我們歡迎第一位 Autoscaler 出場！
Cluster Autoscaler（CA） 負責調整 node-pool 的 node size scaling，屬於 cluster level autoscaler。
 白話文：開新機器，關沒路用的機器 😈
  Scale-up： 有 pod 的狀態是 unschedulable 時 Scale-down： 觀察 pod 總共的 memory/CPU request 是否 < 50%（非真實的 resource utilization）+ 沒有其他 pod/node 的條件限制 可設定 min/maxi poolsize（GKE），自己管理的叢集可以設定更多參數 會參照 PriorityClass 來調控 pod，但就是僅僅設立一條貧窮截止線，當前是 -10 ，autoscaler 不會因為低於此線的 pod 而去 scale-up，需要 scale-down 也不會理會 node 裡面是否有這種 pod 部分設定設不好會讓 CA 沒辦法 scaling  CA 要關 node 然後 evict pod 時違反 pod affinity/anti-affinity 和 PodDisruptionBudget 在 node 加上 annotation 可防止被 scale down：&#34;cluster-autoscaler."><meta name=theme-color content="#ffcd00"><meta property="og:title" content="Kuberenetes Autoscaling 相關知識小整理 • Weihang Lo"><meta property="og:description" content="K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬
本文以重點整理的方式，先介紹目前常見的 Autoscaler，再介紹一些防止 pod 被亂殺的 config。
（撰於 2020-03-23，基於 Kubernetes 1.17，但 Api Versions 太多請自行查閱手冊）
讓我們歡迎第一位 Autoscaler 出場！
Cluster Autoscaler（CA） 負責調整 node-pool 的 node size scaling，屬於 cluster level autoscaler。
 白話文：開新機器，關沒路用的機器 😈
  Scale-up： 有 pod 的狀態是 unschedulable 時 Scale-down： 觀察 pod 總共的 memory/CPU request 是否 < 50%（非真實的 resource utilization）+ 沒有其他 pod/node 的條件限制 可設定 min/maxi poolsize（GKE），自己管理的叢集可以設定更多參數 會參照 PriorityClass 來調控 pod，但就是僅僅設立一條貧窮截止線，當前是 -10 ，autoscaler 不會因為低於此線的 pod 而去 scale-up，需要 scale-down 也不會理會 node 裡面是否有這種 pod 部分設定設不好會讓 CA 沒辦法 scaling  CA 要關 node 然後 evict pod 時違反 pod affinity/anti-affinity 和 PodDisruptionBudget 在 node 加上 annotation 可防止被 scale down：&#34;cluster-autoscaler."><meta property="og:url" content="https://weihanglo.tw/posts/2020/k8s-autoscaling/"><meta property="og:site_name" content="Weihang Lo"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="DevOps"><meta property="article:published_time" content="2020-03-23T00:00:00+08:00"><meta property="article:modified_time" content="2020-03-23T00:00:00+08:00"><meta name=twitter:card content="summary"><meta name=generator content="Hugo 0.62.2"><title>Kuberenetes Autoscaling 相關知識小整理 • Weihang Lo</title><link rel=canonical href=https://weihanglo.tw/posts/2020/k8s-autoscaling/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/assets/css/main.6a060eb7.css><link rel=stylesheet href=/css/custom.css><style>:root{--color-accent:#ffcd00}</style></head><body class="page type-posts"><div class=site><a class=screen-reader-text href=#content>Skip to Content</a><div class=main><nav id=main-menu class="menu main-menu" aria-label="Main Menu"><div class=container><ul><li class=item><a href=/>Home</a></li><li class=item><a href=/posts/>Posts</a></li><li class=item><a href=/tags/>Tags</a></li><li class=item><a href=/about/>About</a></li></ul></div></nav><div class=header-widgets><div class=container></div></div><header id=header class="header site-header"><div class="container sep-after"><div class=header-info><p class="site-title title">Weihang Lo</p><p class="desc site-desc"></p></div></div></header><main id=content><article lang=en class=entry><header class="header entry-header"><div class="container sep-after"><div class=header-info><h1 class=title>Kuberenetes Autoscaling 相關知識小整理</h1></div><div class=entry-meta><span class=posted-on><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg><span class=screen-reader-text>Posted on</span>
<time class=entry-date datetime=2020-03-23T00:00:00+08:00>2020, Mar 23</time></span>
<span class=reading-time><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 15 15"/></svg>5 mins read</span></div></div></header><div class="container entry-content"><p>K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬</p><p>本文以重點整理的方式，先介紹目前常見的 Autoscaler，再介紹一些防止 pod 被亂殺的 config。</p><p><em>（撰於 2020-03-23，基於 Kubernetes 1.17，但 Api Versions 太多請自行查閱手冊）</em></p><p>讓我們歡迎第一位 Autoscaler 出場！</p><h2 id=cluster-autoscalercahttpsgithubcomkubernetesautoscalertreemastercluster-autoscaler><a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Cluster Autoscaler（CA）</a></h2><p>負責調整 node-pool 的 node size scaling，屬於 cluster level autoscaler。</p><blockquote><p>白話文：開新機器，關沒路用的機器 😈</p></blockquote><ul><li><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-up-work><strong>Scale-up：</strong></a> 有 pod 的狀態是 <code>unschedulable</code> 時</li><li><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work><strong>Scale-down：</strong></a> 觀察 pod 總共的 memory/CPU request 是否 &lt; 50%（非真實的 resource utilization）+ 沒有其他 pod/node 的條件限制</li><li>可設定 min/maxi poolsize（GKE），自己管理的叢集可以<a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca>設定更多參數</a></li><li>會參照 PriorityClass 來調控 pod，但就是僅僅設立一條<del>貧窮</del>截止線，當前是 <code>-10</code> ，autoscaler 不會因為低於此線的 pod 而去 scale-up，需要 scale-down 也不會理會 node 裡面是否有這種 pod</li><li>部分設定設不好會讓 CA 沒辦法 scaling<ul><li>CA 要關 node 然後 evict pod 時違反 pod affinity/anti-affinity 和 PodDisruptionBudget</li><li>在 node 加上 annotation 可防止被 scale down：<code>"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"</code></li></ul></li><li>理論上 CA 很快，預設發現 <code>unschedulable</code> 10 秒就 scale-up，瓶頸會在 Node provision（開機器）需要數分鐘</li><li>⚠️ GKE 若要 enable CA，會讓 K8s master 暫停重開，不可不謹慎</li><li>⚠️ 手動更改 autoscalable node-pool 的 node label 等欄位會掉，因為修改不會傳播到 node template</li></ul><h2 id=horizontal-pod-autoscalerhpahttpskubernetesiodocstasksrun-applicationhorizontal-pod-autoscale><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler（HPA）</a></h2><p>管理 pod 數量的 autoscaler，屬於 pod level，同時也是一種 K8s API resource。</p><blockquote><p>白話文：自動增減 replica 數量</p></blockquote><ul><li><strong>Scale-up：</strong> 檢查 metrics，發現過了 threshold 就增加 deployment 的 replicas</li><li><strong>Scale-down：</strong> 檢查 metrics，發現過了 threshold 就減少 deployment 的 replicas</li><li>在 scale up/down 之後都會等個三五分鐘穩定後，再開始檢查 metric（如果突然爆漲應該來不及？）</li><li>可以設定 custom/external metrics 來觸發 autoscaling</li><li>⚠️ v2beta2 以上的 HPA 才有 metrics 可以檢查，v1 只能檢查 CPU utilization</li></ul><p><img src=https://d33wubrfki0l68.cloudfront.net/4fe1ef7265a93f5f564bd3fbb0269ebd10b73b4e/1775d/images/docs/horizontal-pod-autoscaler.svg alt></p><p>目前 <code>hpa.spec</code> 有以下的 fields：</p><ul><li><code>maxReplicas</code>：恩，字面上的意思</li><li><code>minReplicas</code>：autoscaler 可以設定的最小 replicas 值，預設為 1</li><li><code>scaleTargetRef</code>：要 scale 的 resource，通常設定為 deployment 等 scheduler（statefulset 不行）</li><li><code>metrics</code>：設定 scaling 要檢查的 metrics，跟 <code>metrics.k8s.io</code> 這些 API object 有關，可設定<ol><li><strong>Resource：</strong> 目標資源的 Memory/CPU 等，通常透過 metrics-server 建立</li><li><strong>Pod：</strong> pod 上的 metrics，不同的是 Resource 預先定義好了，但 Pod 則是可以而外傭 <code>custom.metrics.k8s.io</code> API 來定義</li><li><strong>Object：</strong> 和 Pod 類似，是其他在同個 namespace 下面的 Resource Object 的 metrics，例如 Ingress hit-per-seconds</li><li><strong>External：</strong> 外部的 metrics，例如 load balancer，我們可以透過 <a href=https://github.com/DirectXMan12/k8s-prometheus-adapter>prometheus-adapter</a> 將 Prometheus 的 metrics 格式轉換為 metrics API 的格式，所以大膽向外求援吧</li></ol></li></ul><p>如果想玩玩 HPA，請參考<a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough>官方文件走過一遍</a>，但千萬不要在正式環境亂玩 😈。</p><blockquote><p>去看了 Kuberentes 現在有什麼 stable metrics，得到以下答案，帥</p></blockquote><p><img src=https://media2.giphy.com/media/cKmfTuhx0kyu1e6BkL/giphy.gif alt></p><h2 id=vertical-pod-autoscalervpahttpsgithubcomkubernetesautoscalerblobmastervertical-pod-autoscalerpkgapisautoscalingk8siov1beta2typesgo><a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1beta2/types.go>Vertical Pod Autoscaler（VPA）</a></h2><p>自動推薦並設定最適合 deployment pod 的 resource requests/limits 設定（最低資源需求和可用資源上限），不需要再 runtime 監控透過人工調整到正確的 CPU/Memory。</p><blockquote><p>白話文：是一個能解放你的生產力的推薦系統啦</p></blockquote><ul><li><strong>Scale-up：</strong> 檢查 metrics，發現過了 threshold 就減少 deployment 的 pod template 的 resources.requests，再透過重啟 pod 實際更新</li><li><strong>Scale-down：</strong> 檢查 metrics，發現過了 threshold 就減少 deployment 的 pod template 的 resources.requests，再透過重啟 pod 實際更新</li><li>是一個 CRD（<a href=https://kubernetes.io/docs/concepts/api-extension/custom-resources/>Custom Resource Definition object</a>）</li><li>VPA 不會更動 <code>resources.limit</code>，事實上，在當前 MVP <a href=https://github.com/kubernetes/community/blob/a358faf/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md#recommendation-model>是將 limit 設定為 infinity</a></li><li>VPA 的改動會參考該 pod 的 historic data（嗯，很有自學能力）</li><li>GKE 可以一鍵啟用，但是 API version 改變也要透過 google cloud</li><li>⚠️ VPA 目前不該跟 HPA 一起用（VPA 0.7.1），除非 <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics>HPA 用了 custom metric 來觸發</a></li><li>⚠️ 對，你發現了一定要重啟 pod 才會更新，這是當前 VPA 的限制，如果受不鳥就暫時先別用吧</li><li>⚠️ 目前和 JVM 有點不搭，因為 JVM 隱匿病情不公開透明</li></ul><p>目前 <code>vpa.spec</code> 有以下幾個 fields：</p><ul><li><code>targetRef</code>： 要調整的 resource，通常設定為 deployment 等 scheduler（statefulset 不行）</li><li><code>updatePolicy</code>：會如何調節 Pod，總共有三種模式，但其實你實際只會用到 Recreate<ul><li><code>Off</code>：關，可用來 dry-run</li><li><code>Initial</code>： 只會在 pod creation 是指派資源，之後整個生命週期都沒 autoscaler 的事</li><li><code>Recreate</code>： 會透過 delete/recreate 來調節 pod 整個生命週期的 resources</li><li><code>Auto</code>：自動用任何 updatePolicy 來做事，目前等於 <code>Recreate</code></li></ul></li><li><code>resourcePolicy.containerPolicies</code>： 決定 autoscaler 如何對特定 container 計算資源用量，是一個 array of struct，有幾個 field<ul><li><code>containerName</code>：想對哪個 container 開刀</li><li><code>mode</code>：有 <code>Auto</code> 或 <code>Off</code> 來決定是否對該 container autoscale</li><li><code>minAllowed</code>：推薦資源用量的最低限度</li><li><code>maxAllowed</code>：推薦資源用量的最高限度</li></ul></li></ul><p>因為文件更新太慢，建議直接看 <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1beta2/types.go>VPA 的 source code</a> 來看到底怎麼用，或是看 <a href=https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler>Google 的文件</a>，此外 GKE 可以一鍵開啟 VPA，beta 版的功能公開出來還不加 beta tag，膽大包青天。</p><p>當然，如果你對實作細節和如何推薦很有興趣的話，也可以看最初的 <a href=https://github.com/kubernetes/community/blob/a358faf/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md>design proposal</a>。</p><p><img src=https://i.imgur.com/Nr68ZyD.png alt></p><h2 id=如何讓該死的-pod-死掉不該死的-pod-晚點死>如何讓該死的 Pod 死掉，不該死的 Pod 晚點死</h2><p>以上是 Autoscaler 的介紹，但無論 scale pod 或 node，都可能殺掉部分的 pod，為了保持網路穩定，或是某些服務需要定量的 replica set 存活（例如用到 raft 共識演算法的服務），我們需要一些設定，讓 pod 不會長在不該長的 node，也不會死得不明不白。</p><h3 id=pod-resources-rquests-and-limits>Pod Resources Rquests and Limits</h3><p>K8s 決定 Pod 放在哪，有一個要點：<strong>哪個 Node 的資源夠就往哪放</strong>。嘟嘟好，<code>pod.spec</code> 可以設定 container 需要多少運算資源（CPU、Memory 等）：</p><ul><li><code>containers[].resources.requests</code>：就是 <strong>minimum</strong>，該容器「至少需要多少資源」</li><li><code>containers[].resources.limits</code>：就是 <strong>maximum</strong>，該 容器「至多可使用多少資源」</li></ul><p>Scheduler 會按照 <code>resources.requests</code> 決定 pod 要放在哪，一個 node 上面所有 pod containers 的 requests 總和不能超過該 node 提供的運算資源總量，否則新 Pod 會放不上去該 node。</p><p>那想知道 <code>limits</code> 怎麼計算的嗎？你不想知道但我還是要說：不同的 container runtime 統計運算資源使用量的方式各異，以最常見的 Docker 來說，CPU 用量是「每 100ms 該 container 佔用多少 CPU time」來決定，而 Memory 用量則是用 <a href=https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources><code>docker run</code> 的 <code>--memory</code></a> 來限制，詳情請<a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>參閱官方手冊</a>。</p><p>這裡偷一下 Sysdig 的圖，視覺化來理解 resources requests：</p><p><img src=https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/image11.png alt></p><p><em>image from Sysdig: <a href=https://sysdig.com/blog/kubernetes-limits-requests/>Understanding Kubernetes limits and requests by example</a></em></p><p>總而言之，在 Pod 要被 schedule 到 node，都會參考這些設定來調控，Cluster Autoscaler 也不例外，如果發現 Autoscaler 運作不能，檢查一下這些設定，然後記得 <strong>requests = 下限</strong>，<strong>limit = 上限</strong>準沒錯。</p><blockquote><p>要限制運算資源使用，還有 <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/><code>ResourceQuota</code></a>（限制 namespace 資源使用）和 <a href=https://kubernetes.io/docs/concepts/policy/limit-range/><code>LimitRange</code></a>（）這兩個 API Resources 可用，甚至可以管理 storage，但設定有些複雜，按下不表。</p><p>然後 <a href=https://cloud.google.com/kubernetes-engine/quotas>GKE 會幫你亂加 ResourceQuota</a>，Google 髒髒。</p></blockquote><h3 id=pod-disruption-budgetpdb>Pod Disruption Budget（PDB）</h3><p>PDB 字面上意思是「Pod 的崩潰預算」，說穿了，就是設定Pod 遇到<strong>事故</strong>時<strong>至少要活幾個</strong>和<strong>至多能死幾個</strong>。
還有機房封城 SRE 進不去重開
我們先來定義「事故」。舉凡系統當機、機房斷電、海底電纜被鯊魚咬，或是封城 SRE 無法進去重開機，這些可能會造成機器損毀的事故都是一種 disruption，但這些是「非自願性事故」。當然，K8s 管理員想要 scale down 一個 node 來維護或是省錢也是一種事故，這種就是「自願性事故」，PDB 就是為了防止管理員那天晚上喝了太多酒不小心一次 drain 太多 node 的<strong>最後一道安全鎖</strong>。</p><p>PDB 有下列幾個 fields：</p><ul><li><code>maxUnavailable</code>：恩，剛剛講過，最多能死幾個 pod</li><li><code>minAvailable</code>：至少要有幾個存活</li><li><code>selector</code>：這個你不知道自己 <code>kubectl explain pdb.spec.selector</code> 吧 💩</li></ul><p>講起來 PDB 很簡單，但其實它非常實用，必須細細品嚐，例如你有個用了 distributed service，由於這種用了共識演算法的服務 qurorum 都需要有一定數量（例如 3），所以你可能選擇對 StatefulSet 的 pod 設定 <code>minAvailable: 3</code>；或是你發現你根本不能死任何一個服務，也可以直接設定 <code>maxUnavailable: 0</code>，這樣 K8s 管理員發現沒辦法 drain node 的時候就會來找你，你就可以好好規劃手動調整，一起 DevOps 惹。</p><h3 id=pod-affinityanti-affinity>Pod Affinity/Anti-Affinity</h3><p>Affinity，親和性，如 water affinity 是親水性，Pod affinity 代表「Pod 會趨於被安置在哪一個 node」，anti-affinity 則反之。</p><p>Pod 可以設定兩種 affinity：Node Affinity 和 Pod Affinity。</p><ul><li><strong>Node Affinity：</strong> 透過各種 expression（or、and、in 等）選擇不同的 node 來安置 pod，實際上就是強化版的 node selector，但可以設定各種「required」或「preferred」屬性決定條件是否必須。</li><li><strong>Pod Affinity：</strong> 這是跨 pod 的 affinity，設定上和 Node Affinity 類似，換句話說就是：這個 pod A 已經在這個 Node 上面跑了，所以 pod B 根據 podAffinity 的設定也要（或不要）在這個 node 上面跑。</li></ul><p>由於 Pod Affinity 還有一個 <code>topologyKey</code> 可以指定 Node label，讓 affinity 限制在有同一個 label 的 node 上面運作，這個 <code>topologyKey</code> 因為實作上安全性和效能考量，有諸多限制，請詳閱<a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity>公開說明書</a>。</p><blockquote><ul><li>⚠️ 目前實作上，配合 CA 使用可能<a href=https://github.com/kubernetes/autoscaler/blob/2933517/cluster-autoscaler/FAQ.md#what-are-the-service-level-objectives-for-cluster-autoscaler>造成 CA 慢三個數量級</a>，且設定不好可能無法 scale down，請注意</li><li>其實還有和 affinity 相反的 <a href=https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/>taint/toleration</a>，會確認 Pod 不在特定的 Node 上安置，有機會再談</li></ul></blockquote><h3 id=pod-priority-and-preemption>Pod Priority and Preemption</h3><p>人有貧富貴賤，<del>而 SRE 最賤</del>，Pod 當然也不例外，我們可以透過 PriorityClass 這個 API resource，讓一些 Pod 就是比其他 Pod 還高貴（或低賤）。</p><p>當設定 <code>pod.spec.priorityClassName</code> 後，這個 pod 就獲得階級身分，如果遇上了有些 pod 沒辦法 schedule 到 node 上，priorityClass 較低的 pod 就會被迫踢出 node，讓高級的 pod 先 schedule，就是這麼現實。</p><ul><li>PriorityClass 和 StorageClass 一樣，是 non-namspaced 的資源</li><li><code>priorityclass.value</code>：是一個 integer，越高越優先</li><li><code>priorityclass.preemptionPolicy</code>：是否能夠擠掉現有的 pod<ul><li><code>Never</code>：只能搶排隊在 Pending 但還沒 schedule 的 pod 前面先被安置</li><li><code>PreemptLowerPriority</code>：比較兇，會嘗試幹掉已經 scheduled 的低順位 pod</li></ul></li><li><a href=https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/><code>NonPreemptingPriority</code> feature gate</a> 在 1.15 截止 1.17 都還在 Alpha，預設 <code>false</code></li><li>和 <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md>QoS</a>（由 pod resources.requests/limits 組合出來）是正交關係，互相影響程度低</li><li>⚠️ PriorityClass 對 PDB 的支援是「盡其所能」，也就是說在需要 evict pod 時，會盡力去找可能符合 PDB 的受害者，但如果沒找到，低順位的 pod 還是會被幹掉</li></ul><blockquote><p>事實上 k8s 有預設兩個 PriorityClass，專門給 K8s 核心元件，原則上不要任意動到<a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#interactions-of-pod-priority-and-qos>這些 PriorityClass</a>，也不要搶他們的資源。</p></blockquote><h2 id=小結>小結</h2><p>感謝您耐心看完這篇冗文，來總複習一下：</p><ol><li>Cluster Autoscaler 控制增減 node 機器，舒服</li><li>Horizontal Pod Autoscaler 控制 deployment 的 replicas 數量，舒服</li><li>Verticla Pod Autoscaler 控制 deployment 的 resource requests/limit 數，舒服</li><li>儘量設定 Pod resources requests 與 limit，但這代表要熟悉 app 使用資源多寡，除非用 VPA</li><li>Pod Disruption Budget 是你最後一道鎖，務必設定</li><li>若服務需要強 HA（跨 region/zone、或跨不同實體機房），請設定 (anti-)affinity</li><li>如果你有些 app 高人一等，非常非常重要，重要到可以把別人的 pod 幹掉，請設定 PriorityClass</li></ol><p>切記，若要測試 autoscaling，千萬注意當前在哪個 cluster context，別說我沒提醒，小心弄壞了正式環境的 Kubernetes cluster 😈。</p><h2 id=references>References</h2><ul><li><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>K8s Doc - Horizontal Pod Autoscaler</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>K8s Doc - Manage Compute Resources for Containers</a></li><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/>K8s Doc - Disruptions</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/>K8s Doc - Assign Pods to Nodes</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption>K8s Doc - Pod Priority and Preemption</a></li><li><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md>GitHub - Cluster Autoscaler FAQ</a></li><li><a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>GitHub - Vertical Pod Autoscaler</a></li><li><a href=https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler>GKE - Cluster autoscaler</a></li><li><a href=https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler>GKE - Vertical Pod Autoscaler</a></li></ul></div><footer class=entry-footer><div class="container sep-before"><div class=tags><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2H12l8.59 8.59A2 2 0 0120.59 13.41z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=screen-reader-text>Tags: </span><a class=tag href=/tags/kubernetes/>Kubernetes</a>, <a class=tag href=/tags/devops/>DevOps</a></div></div><div style=text-align:center;padding-top:2em><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/><img src=https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png alt=cc-by-nc-sa-4></a></div></footer></article><nav class=entry-nav><div class=container><div class="prev-entry sep-before"><a href=/posts/2020/www-0x09/><span aria-hidden=true><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="20" y1="12" x2="4" y2="12"/><polyline points="10 18 4 12 10 6"/></svg>Previous</span>
<span class=screen-reader-text>Previous post: </span>WWW 0x09: 到底要不要擔心 blocking</a></div><div class="next-entry sep-before"><a href=/posts/2020/www-0x0a/><span class=screen-reader-text>Next post: </span>WWW 0x0A: 嗯，你這塊 0xDEADBEEF<span aria-hidden=true>Next<svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="4" y1="12" x2="20" y2="12"/><polyline points="14 6 20 12 14 18"/></svg></span></a></div></div></nav></main><footer id=footer class=footer><div class="container sep-before"><section class="widget widget-social_menu sep-after"><nav aria-label="Social Menu"><ul><li><a href=https://github.com/weihanglo target=_blank rel=noopener><span class=screen-reader-text>Open Github account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://facebook.com/weihanglo target=_blank rel=noopener><span class=screen-reader-text>Open Facebook account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M18 2h-3a5 5 0 00-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 011-1h3z"/></svg></a></li><li><a href=https://twitter.com/weihanglo target=_blank rel=noopener><span class=screen-reader-text>Open Twitter account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a></li><li><a href=https://linkedin.com/in/weihanglo target=_blank rel=noopener><span class=screen-reader-text>Open Linkedin account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></li><li><a href=https://t.me/weihanglo target=_blank rel=noopener><span class=screen-reader-text>Open Telegram account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="m22.05 1.577c-.393-.016-.784.08-1.117.235-.484.186-4.92 1.902-9.41 3.64C9.263 6.325 7.005 7.198 5.267 7.867 3.53 8.537 2.222 9.035 2.153 9.059c-.46.16-1.082.362-1.61.984-.79581202 1.058365.21077405 1.964825 1.004 2.499 1.76.564 3.58 1.102 5.087 1.608.556 1.96 1.09 3.927 1.618 5.89.174.394.553.54.944.544l-.002.02s.307.03.606-.042c.3-.07.677-.244 1.02-.565.377-.354 1.4-1.36 1.98-1.928l4.37 3.226.035.02s.484.34 1.192.388c.354.024.82-.044 1.22-.337.403-.294.67-.767.795-1.307.374-1.63 2.853-13.427 3.276-15.38L23.676 4.725C23.972 3.625 23.863 2.617 23.18 2.02 22.838 1.723 22.444 1.593 22.05 1.576z"/></svg></a></li></ul></nav></section><div class=copyright><p>&copy; 2017-2020 Weihang Lo</p></div></div></footer></div></div><script>window.__assets_js_src="/assets/js/"</script><script src=/assets/js/main.67d669ac.js></script><script src=/js/custom.js></script></body></html>