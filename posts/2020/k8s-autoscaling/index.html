<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Kuberenetes Autoscaling 相關知識小整理 | Weihang Lo</title>
<meta name=keywords content="Kubernetes,DevOps">
<meta name=description content="K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬 本文以重點整理的方式，先介紹目前常見的 Autoscal">
<meta name=author content>
<link rel=canonical href=https://weihanglo.tw/posts/2020/k8s-autoscaling/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://weihanglo.tw/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://weihanglo.tw/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://weihanglo.tw/favicon-32x32.png>
<link rel=apple-touch-icon href=https://weihanglo.tw/apple-touch-icon.png>
<link rel=mask-icon href=https://weihanglo.tw/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}],throwOnError:!1})})</script>
<meta property="og:title" content="Kuberenetes Autoscaling 相關知識小整理">
<meta property="og:description" content="K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬 本文以重點整理的方式，先介紹目前常見的 Autoscal">
<meta property="og:type" content="article">
<meta property="og:url" content="https://weihanglo.tw/posts/2020/k8s-autoscaling/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-03-23T00:00:00+08:00">
<meta property="article:modified_time" content="2020-03-23T00:00:00+08:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Kuberenetes Autoscaling 相關知識小整理">
<meta name=twitter:description content="K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬 本文以重點整理的方式，先介紹目前常見的 Autoscal">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://weihanglo.tw/posts/"},{"@type":"ListItem","position":3,"name":"Kuberenetes Autoscaling 相關知識小整理","item":"https://weihanglo.tw/posts/2020/k8s-autoscaling/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kuberenetes Autoscaling 相關知識小整理","name":"Kuberenetes Autoscaling 相關知識小整理","description":"K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬 本文以重點整理的方式，先介紹目前常見的 Autoscal","keywords":["Kubernetes","DevOps"],"articleBody":"K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬\n本文以重點整理的方式，先介紹目前常見的 Autoscaler，再介紹一些防止 pod 被亂殺的 config。\n（撰於 2020-03-23，基於 Kubernetes 1.17，但 Api Versions 太多請自行查閱手冊）\n讓我們歡迎第一位 Autoscaler 出場！\nCluster Autoscaler（CA） 負責調整 node-pool 的 node size scaling，屬於 cluster level autoscaler。\n 白話文：開新機器，關沒路用的機器 😈\n  Scale-up： 有 pod 的狀態是 unschedulable 時 Scale-down： 觀察 pod 總共的 memory/CPU request 是否 可設定 min/maxi poolsize（GKE），自己管理的叢集可以設定更多參數 會參照 PriorityClass 來調控 pod，但就是僅僅設立一條貧窮截止線，當前是 -10 ，autoscaler 不會因為低於此線的 pod 而去 scale-up，需要 scale-down 也不會理會 node 裡面是否有這種 pod 部分設定設不好會讓 CA 沒辦法 scaling  CA 要關 node 然後 evict pod 時違反 pod affinity/anti-affinity 和 PodDisruptionBudget 在 node 加上 annotation 可防止被 scale down：\"cluster-autoscaler.kubernetes.io/scale-down-disabled\": \"true\"   理論上 CA 很快，預設發現 unschedulable 10 秒就 scale-up，瓶頸會在 Node provision（開機器）需要數分鐘 ⚠️ GKE 若要 enable CA，會讓 K8s master 暫停重開，不可不謹慎 ⚠️ 手動更改 autoscalable node-pool 的 node label 等欄位會掉，因為修改不會傳播到 node template  Horizontal Pod Autoscaler（HPA） 管理 pod 數量的 autoscaler，屬於 pod level，同時也是一種 K8s API resource。\n 白話文：自動增減 replica 數量\n  Scale-up： 檢查 metrics，發現過了 threshold 就增加 deployment 的 replicas Scale-down： 檢查 metrics，發現過了 threshold 就減少 deployment 的 replicas 在 scale up/down 之後都會等個三五分鐘穩定後，再開始檢查 metric（如果突然爆漲應該來不及？） 可以設定 custom/external metrics 來觸發 autoscaling ⚠️ v2beta2 以上的 HPA 才有 metrics 可以檢查，v1 只能檢查 CPU utilization  目前 hpa.spec 有以下的 fields：\n maxReplicas：恩，字面上的意思 minReplicas：autoscaler 可以設定的最小 replicas 值，預設為 1 scaleTargetRef：要 scale 的 resource，通常設定為 deployment 等 scheduler（statefulset 不行） metrics：設定 scaling 要檢查的 metrics，跟 metrics.k8s.io 這些 API object 有關，可設定  Resource： 目標資源的 Memory/CPU 等，通常透過 metrics-server 建立 Pod： pod 上的 metrics，不同的是 Resource 預先定義好了，但 Pod 則是可以而外傭 custom.metrics.k8s.io API 來定義 Object： 和 Pod 類似，是其他在同個 namespace 下面的 Resource Object 的 metrics，例如 Ingress hit-per-seconds External： 外部的 metrics，例如 load balancer，我們可以透過 prometheus-adapter 將 Prometheus 的 metrics 格式轉換為 metrics API 的格式，所以大膽向外求援吧    如果想玩玩 HPA，請參考官方文件走過一遍，但千萬不要在正式環境亂玩 😈。\n 去看了 Kuberentes 現在有什麼 stable metrics，得到以下答案，帥\n Vertical Pod Autoscaler（VPA） 自動推薦並設定最適合 deployment pod 的 resource requests/limits 設定（最低資源需求和可用資源上限），不需要再 runtime 監控透過人工調整到正確的 CPU/Memory。\n 白話文：是一個能解放你的生產力的推薦系統啦\n  Scale-up： 檢查 metrics，發現過了 threshold 就減少 deployment 的 pod template 的 resources.requests，再透過重啟 pod 實際更新 Scale-down： 檢查 metrics，發現過了 threshold 就減少 deployment 的 pod template 的 resources.requests，再透過重啟 pod 實際更新 是一個 CRD（Custom Resource Definition object） VPA 不會更動 resources.limit，事實上，在當前 MVP 是將 limit 設定為 infinity VPA 的改動會參考該 pod 的 historic data（嗯，很有自學能力） GKE 可以一鍵啟用，但是 API version 改變也要透過 google cloud ⚠️ VPA 目前不該跟 HPA 一起用（VPA 0.7.1），除非 HPA 用了 custom metric 來觸發 ⚠️ 對，你發現了一定要重啟 pod 才會更新，這是當前 VPA 的限制，如果受不鳥就暫時先別用吧 ⚠️ 目前和 JVM 有點不搭，因為 JVM 隱匿病情不公開透明  目前 vpa.spec 有以下幾個 fields：\n targetRef： 要調整的 resource，通常設定為 deployment 等 scheduler（statefulset 不行） updatePolicy：會如何調節 Pod，總共有三種模式，但其實你實際只會用到 Recreate  Off：關，可用來 dry-run Initial： 只會在 pod creation 是指派資源，之後整個生命週期都沒 autoscaler 的事 Recreate： 會透過 delete/recreate 來調節 pod 整個生命週期的 resources Auto：自動用任何 updatePolicy 來做事，目前等於 Recreate   resourcePolicy.containerPolicies： 決定 autoscaler 如何對特定 container 計算資源用量，是一個 array of struct，有幾個 field  containerName：想對哪個 container 開刀 mode：有 Auto 或 Off 來決定是否對該 container autoscale minAllowed：推薦資源用量的最低限度 maxAllowed：推薦資源用量的最高限度    因為文件更新太慢，建議直接看 VPA 的 source code 來看到底怎麼用，或是看 Google 的文件，此外 GKE 可以一鍵開啟 VPA，beta 版的功能公開出來還不加 beta tag，膽大包青天。\n當然，如果你對實作細節和如何推薦很有興趣的話，也可以看最初的 design proposal。\n如何讓該死的 Pod 死掉，不該死的 Pod 晚點死 以上是 Autoscaler 的介紹，但無論 scale pod 或 node，都可能殺掉部分的 pod，為了保持網路穩定，或是某些服務需要定量的 replica set 存活（例如用到 raft 共識演算法的服務），我們需要一些設定，讓 pod 不會長在不該長的 node，也不會死得不明不白。\nPod Resources Rquests and Limits K8s 決定 Pod 放在哪，有一個要點：哪個 Node 的資源夠就往哪放。嘟嘟好，pod.spec 可以設定 container 需要多少運算資源（CPU、Memory 等）：\n containers[].resources.requests：就是 minimum，該容器「至少需要多少資源」 containers[].resources.limits：就是 maximum，該 容器「至多可使用多少資源」  Scheduler 會按照 resources.requests 決定 pod 要放在哪，一個 node 上面所有 pod containers 的 requests 總和不能超過該 node 提供的運算資源總量，否則新 Pod 會放不上去該 node。\n那想知道 limits 怎麼計算的嗎？你不想知道但我還是要說：不同的 container runtime 統計運算資源使用量的方式各異，以最常見的 Docker 來說，CPU 用量是「每 100ms 該 container 佔用多少 CPU time」來決定，而 Memory 用量則是用 docker run 的 --memory 來限制，詳情請參閱官方手冊。\n這裡偷一下 Sysdig 的圖，視覺化來理解 resources requests：\nimage from Sysdig: Understanding Kubernetes limits and requests by example\n總而言之，在 Pod 要被 schedule 到 node，都會參考這些設定來調控，Cluster Autoscaler 也不例外，如果發現 Autoscaler 運作不能，檢查一下這些設定，然後記得 requests = 下限，limit = 上限準沒錯。\n 要限制運算資源使用，還有 ResourceQuota（限制 namespace 資源使用）和 LimitRange（）這兩個 API Resources 可用，甚至可以管理 storage，但設定有些複雜，按下不表。\n然後 GKE 會幫你亂加 ResourceQuota，Google 髒髒。\n Pod Disruption Budget（PDB） PDB 字面上意思是「Pod 的崩潰預算」，說穿了，就是設定Pod 遇到事故時至少要活幾個和至多能死幾個。 還有機房封城 SRE 進不去重開 我們先來定義「事故」。舉凡系統當機、機房斷電、海底電纜被鯊魚咬，或是封城 SRE 無法進去重開機，這些可能會造成機器損毀的事故都是一種 disruption，但這些是「非自願性事故」。當然，K8s 管理員想要 scale down 一個 node 來維護或是省錢也是一種事故，這種就是「自願性事故」，PDB 就是為了防止管理員那天晚上喝了太多酒不小心一次 drain 太多 node 的最後一道安全鎖。\nPDB 有下列幾個 fields：\n maxUnavailable：恩，剛剛講過，最多能死幾個 pod minAvailable：至少要有幾個存活 selector：這個你不知道自己 kubectl explain pdb.spec.selector 吧 💩  講起來 PDB 很簡單，但其實它非常實用，必須細細品嚐，例如你有個用了 distributed service，由於這種用了共識演算法的服務 qurorum 都需要有一定數量（例如 3），所以你可能選擇對 StatefulSet 的 pod 設定 minAvailable: 3；或是你發現你根本不能死任何一個服務，也可以直接設定 maxUnavailable: 0，這樣 K8s 管理員發現沒辦法 drain node 的時候就會來找你，你就可以好好規劃手動調整，一起 DevOps 惹。\nPod Affinity/Anti-Affinity Affinity，親和性，如 water affinity 是親水性，Pod affinity 代表「Pod 會趨於被安置在哪一個 node」，anti-affinity 則反之。\nPod 可以設定兩種 affinity：Node Affinity 和 Pod Affinity。\n Node Affinity： 透過各種 expression（or、and、in 等）選擇不同的 node 來安置 pod，實際上就是強化版的 node selector，但可以設定各種「required」或「preferred」屬性決定條件是否必須。 Pod Affinity： 這是跨 pod 的 affinity，設定上和 Node Affinity 類似，換句話說就是：這個 pod A 已經在這個 Node 上面跑了，所以 pod B 根據 podAffinity 的設定也要（或不要）在這個 node 上面跑。  由於 Pod Affinity 還有一個 topologyKey 可以指定 Node label，讓 affinity 限制在有同一個 label 的 node 上面運作，這個 topologyKey 因為實作上安全性和效能考量，有諸多限制，請詳閱公開說明書。\n  ⚠️ 目前實作上，配合 CA 使用可能造成 CA 慢三個數量級，且設定不好可能無法 scale down，請注意 其實還有和 affinity 相反的 taint/toleration，會確認 Pod 不在特定的 Node 上安置，有機會再談   Pod Priority and Preemption 人有貧富貴賤，而 SRE 最賤，Pod 當然也不例外，我們可以透過 PriorityClass 這個 API resource，讓一些 Pod 就是比其他 Pod 還高貴（或低賤）。\n當設定 pod.spec.priorityClassName 後，這個 pod 就獲得階級身分，如果遇上了有些 pod 沒辦法 schedule 到 node 上，priorityClass 較低的 pod 就會被迫踢出 node，讓高級的 pod 先 schedule，就是這麼現實。\n PriorityClass 和 StorageClass 一樣，是 non-namspaced 的資源 priorityclass.value：是一個 integer，越高越優先 priorityclass.preemptionPolicy：是否能夠擠掉現有的 pod  Never：只能搶排隊在 Pending 但還沒 schedule 的 pod 前面先被安置 PreemptLowerPriority：比較兇，會嘗試幹掉已經 scheduled 的低順位 pod   NonPreemptingPriority feature gate 在 1.15 截止 1.17 都還在 Alpha，預設 false 和 QoS（由 pod resources.requests/limits 組合出來）是正交關係，互相影響程度低 ⚠️ PriorityClass 對 PDB 的支援是「盡其所能」，也就是說在需要 evict pod 時，會盡力去找可能符合 PDB 的受害者，但如果沒找到，低順位的 pod 還是會被幹掉   事實上 k8s 有預設兩個 PriorityClass，專門給 K8s 核心元件，原則上不要任意動到這些 PriorityClass，也不要搶他們的資源。\n 小結 感謝您耐心看完這篇冗文，來總複習一下：\n Cluster Autoscaler 控制增減 node 機器，舒服 Horizontal Pod Autoscaler 控制 deployment 的 replicas 數量，舒服 Verticla Pod Autoscaler 控制 deployment 的 resource requests/limit 數，舒服 儘量設定 Pod resources requests 與 limit，但這代表要熟悉 app 使用資源多寡，除非用 VPA Pod Disruption Budget 是你最後一道鎖，務必設定 若服務需要強 HA（跨 region/zone、或跨不同實體機房），請設定 (anti-)affinity 如果你有些 app 高人一等，非常非常重要，重要到可以把別人的 pod 幹掉，請設定 PriorityClass  切記，若要測試 autoscaling，千萬注意當前在哪個 cluster context，別說我沒提醒，小心弄壞了正式環境的 Kubernetes cluster 😈。\nReferences  K8s Doc - Horizontal Pod Autoscaler K8s Doc - Manage Compute Resources for Containers K8s Doc - Disruptions K8s Doc - Assign Pods to Nodes K8s Doc - Pod Priority and Preemption GitHub - Cluster Autoscaler FAQ GitHub - Vertical Pod Autoscaler GKE - Cluster autoscaler GKE - Vertical Pod Autoscaler  ","wordCount":"4443","inLanguage":"en","datePublished":"2020-03-23T00:00:00+08:00","dateModified":"2020-03-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://weihanglo.tw/posts/2020/k8s-autoscaling/"},"publisher":{"@type":"Organization","name":"Weihang Lo","logo":{"@type":"ImageObject","url":"https://weihanglo.tw/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://weihanglo.tw accesskey=h title="Weihang Lo (Alt + H)">Weihang Lo</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://weihanglo.tw/ title=Home>
<span>Home</span>
</a>
</li>
<li>
<a href=https://weihanglo.tw/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=https://weihanglo.tw/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://weihanglo.tw/about/ title=About>
<span>About</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Kuberenetes Autoscaling 相關知識小整理
</h1>
<div class=post-meta><span title="2020-03-23 00:00:00 +0800 +0800">March 23, 2020</span>&nbsp;·&nbsp;9 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#cluster-autoscalercahttpsgithubcomkubernetesautoscalertreemastercluster-autoscaler aria-label="Cluster Autoscaler（CA）"><a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Cluster Autoscaler（CA）</a></a></li>
<li>
<a href=#horizontal-pod-autoscalerhpahttpskubernetesiodocstasksrun-applicationhorizontal-pod-autoscale aria-label="Horizontal Pod Autoscaler（HPA）"><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler（HPA）</a></a></li>
<li>
<a href=#vertical-pod-autoscalervpahttpsgithubcomkubernetesautoscalerblobmastervertical-pod-autoscalerpkgapisautoscalingk8siov1beta2typesgo aria-label="Vertical Pod Autoscaler（VPA）"><a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1beta2/types.go>Vertical Pod Autoscaler（VPA）</a></a></li>
<li>
<a href=#%e5%a6%82%e4%bd%95%e8%ae%93%e8%a9%b2%e6%ad%bb%e7%9a%84-pod-%e6%ad%bb%e6%8e%89%e4%b8%8d%e8%a9%b2%e6%ad%bb%e7%9a%84-pod-%e6%99%9a%e9%bb%9e%e6%ad%bb aria-label="如何讓該死的 Pod 死掉，不該死的 Pod 晚點死">如何讓該死的 Pod 死掉，不該死的 Pod 晚點死</a><ul>
<li>
<a href=#pod-resources-rquests-and-limits aria-label="Pod Resources Rquests and Limits">Pod Resources Rquests and Limits</a></li>
<li>
<a href=#pod-disruption-budgetpdb aria-label="Pod Disruption Budget（PDB）">Pod Disruption Budget（PDB）</a></li>
<li>
<a href=#pod-affinityanti-affinity aria-label="Pod Affinity/Anti-Affinity">Pod Affinity/Anti-Affinity</a></li>
<li>
<a href=#pod-priority-and-preemption aria-label="Pod Priority and Preemption">Pod Priority and Preemption</a></li></ul>
</li>
<li>
<a href=#%e5%b0%8f%e7%b5%90 aria-label=小結>小結</a></li>
<li>
<a href=#references aria-label=References>References</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>K8s 有好用的 autoscaling 功能，但你知道除了 pod 之外，node 也可以 auto scaling 嗎？帥，你知道就不用分享了啊 🚬</p>
<p>本文以重點整理的方式，先介紹目前常見的 Autoscaler，再介紹一些防止 pod 被亂殺的 config。</p>
<p><em>（撰於 2020-03-23，基於 Kubernetes 1.17，但 Api Versions 太多請自行查閱手冊）</em></p>
<p>讓我們歡迎第一位 Autoscaler 出場！</p>
<h2 id=cluster-autoscalercahttpsgithubcomkubernetesautoscalertreemastercluster-autoscaler><a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Cluster Autoscaler（CA）</a><a hidden class=anchor aria-hidden=true href=#cluster-autoscalercahttpsgithubcomkubernetesautoscalertreemastercluster-autoscaler>#</a></h2>
<p>負責調整 node-pool 的 node size scaling，屬於 cluster level autoscaler。</p>
<blockquote>
<p>白話文：開新機器，關沒路用的機器 😈</p>
</blockquote>
<ul>
<li><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-up-work><strong>Scale-up：</strong></a> 有 pod 的狀態是 <code>unschedulable</code> 時</li>
<li><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work><strong>Scale-down：</strong></a> 觀察 pod 總共的 memory/CPU request 是否 &lt; 50%（非真實的 resource utilization）+ 沒有其他 pod/node 的條件限制</li>
<li>可設定 min/maxi poolsize（GKE），自己管理的叢集可以<a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca>設定更多參數</a></li>
<li>會參照 PriorityClass 來調控 pod，但就是僅僅設立一條<del>貧窮</del>截止線，當前是 <code>-10</code> ，autoscaler 不會因為低於此線的 pod 而去 scale-up，需要 scale-down 也不會理會 node 裡面是否有這種 pod</li>
<li>部分設定設不好會讓 CA 沒辦法 scaling
<ul>
<li>CA 要關 node 然後 evict pod 時違反 pod affinity/anti-affinity 和 PodDisruptionBudget</li>
<li>在 node 加上 annotation 可防止被 scale down：<code>"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"</code></li>
</ul>
</li>
<li>理論上 CA 很快，預設發現 <code>unschedulable</code> 10 秒就 scale-up，瓶頸會在 Node provision（開機器）需要數分鐘</li>
<li>⚠️ GKE 若要 enable CA，會讓 K8s master 暫停重開，不可不謹慎</li>
<li>⚠️ 手動更改 autoscalable node-pool 的 node label 等欄位會掉，因為修改不會傳播到 node template</li>
</ul>
<h2 id=horizontal-pod-autoscalerhpahttpskubernetesiodocstasksrun-applicationhorizontal-pod-autoscale><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler（HPA）</a><a hidden class=anchor aria-hidden=true href=#horizontal-pod-autoscalerhpahttpskubernetesiodocstasksrun-applicationhorizontal-pod-autoscale>#</a></h2>
<p>管理 pod 數量的 autoscaler，屬於 pod level，同時也是一種 K8s API resource。</p>
<blockquote>
<p>白話文：自動增減 replica 數量</p>
</blockquote>
<ul>
<li><strong>Scale-up：</strong> 檢查 metrics，發現過了 threshold 就增加 deployment 的 replicas</li>
<li><strong>Scale-down：</strong> 檢查 metrics，發現過了 threshold 就減少 deployment 的 replicas</li>
<li>在 scale up/down 之後都會等個三五分鐘穩定後，再開始檢查 metric（如果突然爆漲應該來不及？）</li>
<li>可以設定 custom/external metrics 來觸發 autoscaling</li>
<li>⚠️ v2beta2 以上的 HPA 才有 metrics 可以檢查，v1 只能檢查 CPU utilization</li>
</ul>
<p><img loading=lazy src=https://d33wubrfki0l68.cloudfront.net/4fe1ef7265a93f5f564bd3fbb0269ebd10b73b4e/1775d/images/docs/horizontal-pod-autoscaler.svg alt>
</p>
<p>目前 <code>hpa.spec</code> 有以下的 fields：</p>
<ul>
<li><code>maxReplicas</code>：恩，字面上的意思</li>
<li><code>minReplicas</code>：autoscaler 可以設定的最小 replicas 值，預設為 1</li>
<li><code>scaleTargetRef</code>：要 scale 的 resource，通常設定為 deployment 等 scheduler（statefulset 不行）</li>
<li><code>metrics</code>：設定 scaling 要檢查的 metrics，跟 <code>metrics.k8s.io</code> 這些 API object 有關，可設定
<ol>
<li><strong>Resource：</strong> 目標資源的 Memory/CPU 等，通常透過 metrics-server 建立</li>
<li><strong>Pod：</strong> pod 上的 metrics，不同的是 Resource 預先定義好了，但 Pod 則是可以而外傭 <code>custom.metrics.k8s.io</code> API 來定義</li>
<li><strong>Object：</strong> 和 Pod 類似，是其他在同個 namespace 下面的 Resource Object 的 metrics，例如 Ingress hit-per-seconds</li>
<li><strong>External：</strong> 外部的 metrics，例如 load balancer，我們可以透過 <a href=https://github.com/DirectXMan12/k8s-prometheus-adapter>prometheus-adapter</a> 將 Prometheus 的 metrics 格式轉換為 metrics API 的格式，所以大膽向外求援吧</li>
</ol>
</li>
</ul>
<p>如果想玩玩 HPA，請參考<a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough>官方文件走過一遍</a>，但千萬不要在正式環境亂玩 😈。</p>
<blockquote>
<p>去看了 Kuberentes 現在有什麼 stable metrics，得到以下答案，帥</p>
</blockquote>
<p><img loading=lazy src=https://media2.giphy.com/media/cKmfTuhx0kyu1e6BkL/giphy.gif alt>
</p>
<h2 id=vertical-pod-autoscalervpahttpsgithubcomkubernetesautoscalerblobmastervertical-pod-autoscalerpkgapisautoscalingk8siov1beta2typesgo><a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1beta2/types.go>Vertical Pod Autoscaler（VPA）</a><a hidden class=anchor aria-hidden=true href=#vertical-pod-autoscalervpahttpsgithubcomkubernetesautoscalerblobmastervertical-pod-autoscalerpkgapisautoscalingk8siov1beta2typesgo>#</a></h2>
<p>自動推薦並設定最適合 deployment pod 的 resource requests/limits 設定（最低資源需求和可用資源上限），不需要再 runtime 監控透過人工調整到正確的 CPU/Memory。</p>
<blockquote>
<p>白話文：是一個能解放你的生產力的推薦系統啦</p>
</blockquote>
<ul>
<li><strong>Scale-up：</strong> 檢查 metrics，發現過了 threshold 就減少 deployment 的 pod template 的 resources.requests，再透過重啟 pod 實際更新</li>
<li><strong>Scale-down：</strong> 檢查 metrics，發現過了 threshold 就減少 deployment 的 pod template 的 resources.requests，再透過重啟 pod 實際更新</li>
<li>是一個 CRD（<a href=https://kubernetes.io/docs/concepts/api-extension/custom-resources/>Custom Resource Definition object</a>）</li>
<li>VPA 不會更動 <code>resources.limit</code>，事實上，在當前 MVP <a href=https://github.com/kubernetes/community/blob/a358faf/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md#recommendation-model>是將 limit 設定為 infinity</a></li>
<li>VPA 的改動會參考該 pod 的 historic data（嗯，很有自學能力）</li>
<li>GKE 可以一鍵啟用，但是 API version 改變也要透過 google cloud</li>
<li>⚠️ VPA 目前不該跟 HPA 一起用（VPA 0.7.1），除非 <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics>HPA 用了 custom metric 來觸發</a></li>
<li>⚠️ 對，你發現了一定要重啟 pod 才會更新，這是當前 VPA 的限制，如果受不鳥就暫時先別用吧</li>
<li>⚠️ 目前和 JVM 有點不搭，因為 JVM 隱匿病情不公開透明</li>
</ul>
<p>目前 <code>vpa.spec</code> 有以下幾個 fields：</p>
<ul>
<li><code>targetRef</code>： 要調整的 resource，通常設定為 deployment 等 scheduler（statefulset 不行）</li>
<li><code>updatePolicy</code>：會如何調節 Pod，總共有三種模式，但其實你實際只會用到 Recreate
<ul>
<li><code>Off</code>：關，可用來 dry-run</li>
<li><code>Initial</code>： 只會在 pod creation 是指派資源，之後整個生命週期都沒 autoscaler 的事</li>
<li><code>Recreate</code>： 會透過 delete/recreate 來調節 pod 整個生命週期的 resources</li>
<li><code>Auto</code>：自動用任何 updatePolicy 來做事，目前等於 <code>Recreate</code></li>
</ul>
</li>
<li><code>resourcePolicy.containerPolicies</code>： 決定 autoscaler 如何對特定 container 計算資源用量，是一個 array of struct，有幾個 field
<ul>
<li><code>containerName</code>：想對哪個 container 開刀</li>
<li><code>mode</code>：有 <code>Auto</code> 或 <code>Off</code> 來決定是否對該 container autoscale</li>
<li><code>minAllowed</code>：推薦資源用量的最低限度</li>
<li><code>maxAllowed</code>：推薦資源用量的最高限度</li>
</ul>
</li>
</ul>
<p>因為文件更新太慢，建議直接看 <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1beta2/types.go>VPA 的 source code</a> 來看到底怎麼用，或是看 <a href=https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler>Google 的文件</a>，此外 GKE 可以一鍵開啟 VPA，beta 版的功能公開出來還不加 beta tag，膽大包青天。</p>
<p>當然，如果你對實作細節和如何推薦很有興趣的話，也可以看最初的 <a href=https://github.com/kubernetes/community/blob/a358faf/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md>design proposal</a>。</p>
<p><img loading=lazy src=https://i.imgur.com/Nr68ZyD.png alt>
</p>
<h2 id=如何讓該死的-pod-死掉不該死的-pod-晚點死>如何讓該死的 Pod 死掉，不該死的 Pod 晚點死<a hidden class=anchor aria-hidden=true href=#如何讓該死的-pod-死掉不該死的-pod-晚點死>#</a></h2>
<p>以上是 Autoscaler 的介紹，但無論 scale pod 或 node，都可能殺掉部分的 pod，為了保持網路穩定，或是某些服務需要定量的 replica set 存活（例如用到 raft 共識演算法的服務），我們需要一些設定，讓 pod 不會長在不該長的 node，也不會死得不明不白。</p>
<h3 id=pod-resources-rquests-and-limits>Pod Resources Rquests and Limits<a hidden class=anchor aria-hidden=true href=#pod-resources-rquests-and-limits>#</a></h3>
<p>K8s 決定 Pod 放在哪，有一個要點：<strong>哪個 Node 的資源夠就往哪放</strong>。嘟嘟好，<code>pod.spec</code> 可以設定 container 需要多少運算資源（CPU、Memory 等）：</p>
<ul>
<li><code>containers[].resources.requests</code>：就是 <strong>minimum</strong>，該容器「至少需要多少資源」</li>
<li><code>containers[].resources.limits</code>：就是 <strong>maximum</strong>，該 容器「至多可使用多少資源」</li>
</ul>
<p>Scheduler 會按照 <code>resources.requests</code> 決定 pod 要放在哪，一個 node 上面所有 pod containers 的 requests 總和不能超過該 node 提供的運算資源總量，否則新 Pod 會放不上去該 node。</p>
<p>那想知道 <code>limits</code> 怎麼計算的嗎？你不想知道但我還是要說：不同的 container runtime 統計運算資源使用量的方式各異，以最常見的 Docker 來說，CPU 用量是「每 100ms 該 container 佔用多少 CPU time」來決定，而 Memory 用量則是用 <a href=https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources><code>docker run</code> 的 <code>--memory</code></a> 來限制，詳情請<a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>參閱官方手冊</a>。</p>
<p>這裡偷一下 Sysdig 的圖，視覺化來理解 resources requests：</p>
<p><img loading=lazy src=https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/image11.png alt>
</p>
<p><em>image from Sysdig: <a href=https://sysdig.com/blog/kubernetes-limits-requests/>Understanding Kubernetes limits and requests by example</a></em></p>
<p>總而言之，在 Pod 要被 schedule 到 node，都會參考這些設定來調控，Cluster Autoscaler 也不例外，如果發現 Autoscaler 運作不能，檢查一下這些設定，然後記得 <strong>requests = 下限</strong>，<strong>limit = 上限</strong>準沒錯。</p>
<blockquote>
<p>要限制運算資源使用，還有 <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/><code>ResourceQuota</code></a>（限制 namespace 資源使用）和 <a href=https://kubernetes.io/docs/concepts/policy/limit-range/><code>LimitRange</code></a>（）這兩個 API Resources 可用，甚至可以管理 storage，但設定有些複雜，按下不表。</p>
<p>然後 <a href=https://cloud.google.com/kubernetes-engine/quotas>GKE 會幫你亂加 ResourceQuota</a>，Google 髒髒。</p>
</blockquote>
<h3 id=pod-disruption-budgetpdb>Pod Disruption Budget（PDB）<a hidden class=anchor aria-hidden=true href=#pod-disruption-budgetpdb>#</a></h3>
<p>PDB 字面上意思是「Pod 的崩潰預算」，說穿了，就是設定Pod 遇到<strong>事故</strong>時<strong>至少要活幾個</strong>和<strong>至多能死幾個</strong>。
還有機房封城 SRE 進不去重開
我們先來定義「事故」。舉凡系統當機、機房斷電、海底電纜被鯊魚咬，或是封城 SRE 無法進去重開機，這些可能會造成機器損毀的事故都是一種 disruption，但這些是「非自願性事故」。當然，K8s 管理員想要 scale down 一個 node 來維護或是省錢也是一種事故，這種就是「自願性事故」，PDB 就是為了防止管理員那天晚上喝了太多酒不小心一次 drain 太多 node 的<strong>最後一道安全鎖</strong>。</p>
<p>PDB 有下列幾個 fields：</p>
<ul>
<li><code>maxUnavailable</code>：恩，剛剛講過，最多能死幾個 pod</li>
<li><code>minAvailable</code>：至少要有幾個存活</li>
<li><code>selector</code>：這個你不知道自己 <code>kubectl explain pdb.spec.selector</code> 吧 💩</li>
</ul>
<p>講起來 PDB 很簡單，但其實它非常實用，必須細細品嚐，例如你有個用了 distributed service，由於這種用了共識演算法的服務 qurorum 都需要有一定數量（例如 3），所以你可能選擇對 StatefulSet 的 pod 設定 <code>minAvailable: 3</code>；或是你發現你根本不能死任何一個服務，也可以直接設定 <code>maxUnavailable: 0</code>，這樣 K8s 管理員發現沒辦法 drain node 的時候就會來找你，你就可以好好規劃手動調整，一起 DevOps 惹。</p>
<h3 id=pod-affinityanti-affinity>Pod Affinity/Anti-Affinity<a hidden class=anchor aria-hidden=true href=#pod-affinityanti-affinity>#</a></h3>
<p>Affinity，親和性，如 water affinity 是親水性，Pod affinity 代表「Pod 會趨於被安置在哪一個 node」，anti-affinity 則反之。</p>
<p>Pod 可以設定兩種 affinity：Node Affinity 和 Pod Affinity。</p>
<ul>
<li><strong>Node Affinity：</strong> 透過各種 expression（or、and、in 等）選擇不同的 node 來安置 pod，實際上就是強化版的 node selector，但可以設定各種「required」或「preferred」屬性決定條件是否必須。</li>
<li><strong>Pod Affinity：</strong> 這是跨 pod 的 affinity，設定上和 Node Affinity 類似，換句話說就是：這個 pod A 已經在這個 Node 上面跑了，所以 pod B 根據 podAffinity 的設定也要（或不要）在這個 node 上面跑。</li>
</ul>
<p>由於 Pod Affinity 還有一個 <code>topologyKey</code> 可以指定 Node label，讓 affinity 限制在有同一個 label 的 node 上面運作，這個 <code>topologyKey</code> 因為實作上安全性和效能考量，有諸多限制，請詳閱<a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity>公開說明書</a>。</p>
<blockquote>
<ul>
<li>⚠️ 目前實作上，配合 CA 使用可能<a href=https://github.com/kubernetes/autoscaler/blob/2933517/cluster-autoscaler/FAQ.md#what-are-the-service-level-objectives-for-cluster-autoscaler>造成 CA 慢三個數量級</a>，且設定不好可能無法 scale down，請注意</li>
<li>其實還有和 affinity 相反的 <a href=https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/>taint/toleration</a>，會確認 Pod 不在特定的 Node 上安置，有機會再談</li>
</ul>
</blockquote>
<h3 id=pod-priority-and-preemption>Pod Priority and Preemption<a hidden class=anchor aria-hidden=true href=#pod-priority-and-preemption>#</a></h3>
<p>人有貧富貴賤，<del>而 SRE 最賤</del>，Pod 當然也不例外，我們可以透過 PriorityClass 這個 API resource，讓一些 Pod 就是比其他 Pod 還高貴（或低賤）。</p>
<p>當設定 <code>pod.spec.priorityClassName</code> 後，這個 pod 就獲得階級身分，如果遇上了有些 pod 沒辦法 schedule 到 node 上，priorityClass 較低的 pod 就會被迫踢出 node，讓高級的 pod 先 schedule，就是這麼現實。</p>
<ul>
<li>PriorityClass 和 StorageClass 一樣，是 non-namspaced 的資源</li>
<li><code>priorityclass.value</code>：是一個 integer，越高越優先</li>
<li><code>priorityclass.preemptionPolicy</code>：是否能夠擠掉現有的 pod
<ul>
<li><code>Never</code>：只能搶排隊在 Pending 但還沒 schedule 的 pod 前面先被安置</li>
<li><code>PreemptLowerPriority</code>：比較兇，會嘗試幹掉已經 scheduled 的低順位 pod</li>
</ul>
</li>
<li><a href=https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/><code>NonPreemptingPriority</code> feature gate</a> 在 1.15 截止 1.17 都還在 Alpha，預設 <code>false</code></li>
<li>和 <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md>QoS</a>（由 pod resources.requests/limits 組合出來）是正交關係，互相影響程度低</li>
<li>⚠️ PriorityClass 對 PDB 的支援是「盡其所能」，也就是說在需要 evict pod 時，會盡力去找可能符合 PDB 的受害者，但如果沒找到，低順位的 pod 還是會被幹掉</li>
</ul>
<blockquote>
<p>事實上 k8s 有預設兩個 PriorityClass，專門給 K8s 核心元件，原則上不要任意動到<a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#interactions-of-pod-priority-and-qos>這些 PriorityClass</a>，也不要搶他們的資源。</p>
</blockquote>
<h2 id=小結>小結<a hidden class=anchor aria-hidden=true href=#小結>#</a></h2>
<p>感謝您耐心看完這篇冗文，來總複習一下：</p>
<ol>
<li>Cluster Autoscaler 控制增減 node 機器，舒服</li>
<li>Horizontal Pod Autoscaler 控制 deployment 的 replicas 數量，舒服</li>
<li>Verticla Pod Autoscaler 控制 deployment 的 resource requests/limit 數，舒服</li>
<li>儘量設定 Pod resources requests 與 limit，但這代表要熟悉 app 使用資源多寡，除非用 VPA</li>
<li>Pod Disruption Budget 是你最後一道鎖，務必設定</li>
<li>若服務需要強 HA（跨 region/zone、或跨不同實體機房），請設定 (anti-)affinity</li>
<li>如果你有些 app 高人一等，非常非常重要，重要到可以把別人的 pod 幹掉，請設定 PriorityClass</li>
</ol>
<p>切記，若要測試 autoscaling，千萬注意當前在哪個 cluster context，別說我沒提醒，小心弄壞了正式環境的 Kubernetes cluster 😈。</p>
<h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2>
<ul>
<li><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>K8s Doc - Horizontal Pod Autoscaler</a></li>
<li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>K8s Doc - Manage Compute Resources for Containers</a></li>
<li><a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/>K8s Doc - Disruptions</a></li>
<li><a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/>K8s Doc - Assign Pods to Nodes</a></li>
<li><a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption>K8s Doc - Pod Priority and Preemption</a></li>
<li><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md>GitHub - Cluster Autoscaler FAQ</a></li>
<li><a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>GitHub - Vertical Pod Autoscaler</a></li>
<li><a href=https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler>GKE - Cluster autoscaler</a></li>
<li><a href=https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler>GKE - Vertical Pod Autoscaler</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://weihanglo.tw/tags/kubernetes/>Kubernetes</a></li>
<li><a href=https://weihanglo.tw/tags/devops/>DevOps</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://weihanglo.tw/posts/2020/www-0x0a/>
<span class=title>« Prev Page</span>
<br>
<span>WWW 0x0A: 嗯，你這塊 0xDEADBEEF</span>
</a>
<a class=next href=https://weihanglo.tw/posts/2020/www-0x09/>
<span class=title>Next Page »</span>
<br>
<span>WWW 0x09: 到底要不要擔心 blocking</span>
</a>
</nav>
</footer>
</article>
</main>
<footer class=footer>
<span>CC BY-NC-SA 4.0</span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>